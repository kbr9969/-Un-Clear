{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZLpjzRS2OLK"
   },
   "source": [
    "# **ESRGAN implementation**\n",
    "-Final implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-MH2az12YLJ"
   },
   "source": [
    "**Importing necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Add,Concatenate,LeakyReLU,Conv2D,Lambda,UpSampling2D\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXJnOFHG2nHa"
   },
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YJ1yiej6Q0L"
   },
   "outputs": [],
   "source": [
    "PATH = \"C:\\\\Users\\\\kbr91\\\\Documents\\\\archive\\\\data\"\n",
    "EPOCHS = 5\n",
    "\n",
    "def random_crop(input_image):\n",
    "    start_height = np.random.randint(0,input_image.shape[0]-96)\n",
    "    start_width = np.random.randint(0,input_image.shape[1]-96)\n",
    "    image = input_image[start_height:start_height+96 , start_width:start_width+96]\n",
    "\n",
    "    return image\n",
    "\n",
    "def load_hr(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_png(image)\n",
    "    image = np.asarray(image)\n",
    "    hr_image = random_crop(image)\n",
    "\n",
    "    return hr_image\n",
    "\n",
    "def load_lr(hr_image):\n",
    "    lr_image = cv2.blur(hr_image,(3,3))\n",
    "    lr_image = cv2.resize(lr_image, (24,24))\n",
    "\n",
    "    return lr_image\n",
    "\n",
    "def normalize(image):\n",
    "    image_t = tf.convert_to_tensor(image , dtype = tf.float32)\n",
    "    image_t = image_t/127.5 -1\n",
    "    return image_t\n",
    "\n",
    "\n",
    "model_training_dataset = os.listdir(PATH)\n",
    "for i in range(len(model_training_dataset)):\n",
    "  model_training_dataset[i] = PATH + '/'+model_training_dataset[i]\n",
    "model_training_hr_dataset = list(map(load_hr, model_training_dataset))\n",
    "model_training_lr_dataset = list(map(load_lr,model_training_hr_dataset))\n",
    "model_training_hr_dataset = tf.convert_to_tensor(list(map(normalize , model_training_hr_dataset)))\n",
    "model_training_lr_dataset = tf.convert_to_tensor(list(map(normalize , model_training_lr_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKTNwKVuFwkC"
   },
   "outputs": [],
   "source": [
    "#Just to check some of the photo processed\n",
    "for i in range(5):\n",
    "    plt.imshow(model_training_hr_dataset[i]/2 + 0.5)\n",
    "    plt.show()\n",
    "    plt.imshow(model_training_lr_dataset[i]/2 + 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA6hkDjUXh5T"
   },
   "source": [
    "**Building the Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQvsE934_1qt"
   },
   "outputs": [],
   "source": [
    "def dense_block(inpt):\n",
    "    b1 = Conv2D(64, kernel_size=3, strides=1, padding='same')(inpt)\n",
    "    b1 = LeakyReLU(0.2)(b1)\n",
    "    b1 = Concatenate()([inpt,b1])\n",
    "    b2 = Conv2D(64, kernel_size=3, strides=1, padding='same')(b1)\n",
    "    b2 = LeakyReLU(0.2)(b2)\n",
    "    b2 = Concatenate()([inpt,b1,b2]) \n",
    "    b5 = Conv2D(64, kernel_size=3, strides=1, padding='same')(b2)\n",
    "    b5 = b5*0.2\n",
    "    b5 = Add()([b5, inpt])\n",
    "    return b5\n",
    "\n",
    "def RRDB(inpt):\n",
    "    x = dense_block(inpt)\n",
    "    x = dense_block(x)\n",
    "    x = x*0.2\n",
    "    out = Add()([x,inpt])\n",
    "\n",
    "    return out\n",
    "\n",
    "def buildGenerator():\n",
    "  inpt = tf.keras.Input(shape = [24,24,3])\n",
    "  up = UpSampling2D(4)(inpt)\n",
    "  conv1 =Conv2D(64 , kernel_size = 3 , strides = 1,padding = 'same')(up)\n",
    "  rrdb = RRDB(conv1)\n",
    "  conv = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same',activation = 'relu')(rrdb)\n",
    "  out = Conv2D(3 , kernel_size= 3 , strides = 1 , padding = 'same')(conv)\n",
    "  return tf.keras.Model(inputs = inpt , outputs = out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZH0oTpXhNk9",
    "outputId": "3e2d45c7-a4c7-424c-fa5a-a7d216ea5d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 24, 24, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 96, 96, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 96, 96, 64)   1792        up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 96, 96, 64)   36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 96, 96, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96, 96, 128)  0           conv2d[0][0]                     \n",
      "                                                                 leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 96, 96, 64)   73792       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96, 96, 256)  0           conv2d[0][0]                     \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 96, 96, 64)   147520      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 96, 96, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 96, 96, 64)   0           tf.math.multiply[0][0]           \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 96, 96, 64)   36928       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 96, 96, 128)  0           add[0][0]                        \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 96, 96, 64)   73792       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 96, 96, 256)  0           add[0][0]                        \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 96, 96, 64)   147520      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 96, 96, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 96, 96, 64)   0           tf.math.multiply_1[0][0]         \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 96, 96, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 96, 96, 64)   0           tf.math.multiply_2[0][0]         \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 96, 96, 256)  147712      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 96, 96, 3)    6915        conv2d_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 672,899\n",
      "Trainable params: 672,899\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model = buildGenerator()\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "626l7twu4B_d"
   },
   "source": [
    "**Building the Relativistic Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emLAxWpnL3kY"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def build_disc_model():\n",
    "    \n",
    "    \n",
    "    leakyrelu_alpha = 0.2\n",
    "    momentum = 0.8\n",
    "\n",
    "    input_0 = tf.keras.layers.Input(shape=(24,24,3))\n",
    "    input_0_upscale = UpSampling2D(4)(input_0)\n",
    "\n",
    "    input_1 = tf.keras.layers.Input(shape=(96,96,3))\n",
    "    input_2 = tf.keras.layers.Input(shape = (96,96,3))\n",
    "\n",
    "    x = tf.keras.layers.concatenate([input_0_upscale,input_1])\n",
    "    y = tf.keras.layers.concatenate([input_0_upscale,input_2])\n",
    "    for i in range(4):\n",
    "      x = Conv2D(64 , kernel_size = 6 , strides = 1 , padding = 'same')(x)\n",
    "      y = Conv2D(64 , kernel_size = 6 , strides = 1 , padding = 'same')(y)\n",
    "      x = LeakyReLU()(x)\n",
    "      y = LeakyReLU()(y)\n",
    "      x = tf.keras.layers.BatchNormalization()(x)\n",
    "      y = tf.keras.layers.BatchNormalization()(y)\n",
    "\n",
    "    logits = x-K.mean(y)\n",
    "    # fully connected layer \n",
    "    output = Conv2D(1,4, activation='sigmoid' , padding = 'same')(logits)   \n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_0 , input_1,input_2], outputs=[output], name='disc_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uRXrWJNSbjbW",
    "outputId": "73854e89-04cf-4e7f-b63f-3ff513fc27fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24, 24, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 96, 96, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 96, 96, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 96, 96, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 96, 96, 6)    0           up_sampling2d_1[0][0]            \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 96, 96, 6)    0           up_sampling2d_1[0][0]            \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 96, 96, 64)   13888       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 96, 96, 64)   13888       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 96, 96, 64)   256         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 96, 96, 64)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 96, 96, 64)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda ()                   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 96, 96, 64)   0           batch_normalization_6[0][0]      \n",
      "                                                                 tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 96, 96, 1)    1025        tf.math.subtract[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 915,969\n",
      "Trainable params: 914,945\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_model = build_disc_model()\n",
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-sjZvB834It"
   },
   "source": [
    "**Building the ESRGAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLl4fzdTbmr0"
   },
   "outputs": [],
   "source": [
    "def relativistic_loss(disc_real,disc_gen):\n",
    "    real = disc_real\n",
    "    fake = disc_gen\n",
    "    fake_logits = K.sigmoid(fake - K.mean(real))\n",
    "    real_logits = K.sigmoid(real - K.mean(fake))\n",
    "            \n",
    "    return [fake_logits, real_logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhVHZ9LjcY5l"
   },
   "outputs": [],
   "source": [
    "\n",
    "def disc_model_loss(fake_logits , real_logits) :\n",
    "  return  K.mean(K.binary_crossentropy(K.zeros_like(fake_logits),fake_logits)+K.binary_crossentropy(K.ones_like(real_logits),real_logits))\n",
    "\n",
    "def gen_model_loss(fake_logits , real_logits) :\n",
    "  return  K.mean(K.binary_crossentropy(K.zeros_like(real_logits),real_logits)+K.binary_crossentropy(K.ones_like(fake_logits),fake_logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mBeemIMxRdv",
    "outputId": "f5534a5b-73cd-4536-ffed-ebd56d413e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "gen_model_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "disc_model_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "\n",
    "# The VGG model for VGG loss , made for our input shape \n",
    "vgg = VGG19(include_top = False, input_shape=(96,96,3))\n",
    "\n",
    "Lambda = 0.05\n",
    "Eeta = 1 #both these values are supposed to be changed after model_traininging_cycles. The initial values are such that the GAN first predicts a rough figure about the images\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "def model_training_step(input_lr_image, target, epoch):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = gen_model(input_lr_image, model_traininging=True)\n",
    "\n",
    "        real_logits  = disc_model([input_lr_image, target , gen_output], model_traininging=True)\n",
    "        fake_logits  = disc_model([input_lr_image, target , gen_output], model_traininging=True)\n",
    "        gen_loss     = Lambda*gen_model_loss(fake_logits, real_logits)\n",
    "        gen_loss    += Eeta*tf.reduce_mean(tf.abs(target - gen_output))\n",
    "        feature_gen  = vgg(preprocess_input(gen_output))\n",
    "        feature_real = vgg(preprocess_input(np.copy(target)))\n",
    "        vgg_loss     = tf.keras.losses.mean_squared_error(feature_gen , feature_real)\n",
    "        gen_loss    += 100*vgg_loss\n",
    "        disc_loss = disc_model_loss(fake_logits, real_logits)\n",
    "\n",
    "    gen_model_gradients = gen_tape.gradient(gen_loss,\n",
    "                                          gen_model.model_trainingable_variables)\n",
    "    disc_model_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               disc_model.model_trainingable_variables)\n",
    "\n",
    "    gen_model_optimizer.apply_gradients(zip(gen_model_gradients,\n",
    "                                          gen_model.model_trainingable_variables))\n",
    "    disc_model_optimizer.apply_gradients(zip(disc_model_gradients,\n",
    "                                              disc_model.model_trainingable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0F8SKiyY3vnk"
   },
   "outputs": [],
   "source": [
    "def fit(model_training_lr,model_training_hr, model_traininging_cycles):\n",
    "    for epoch in range(model_traininging_cycles):\n",
    "        start = time.time()\n",
    "        print(\".\")\n",
    "        for i in range(50):\n",
    "          input_image  = model_training_lr[4*i:4*i+4]\n",
    "          target  =  model_training_hr[4*i:4*i+4]\n",
    "          model_training_step(input_image,target , epoch)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        generated = gen_model(model_training_lr_dataset[0:5])\n",
    "        for i in range(len(generated)):\n",
    "          plt.subplot(1,3,1)\n",
    "          plt.imshow(model_training_lr_dataset[i]/2+0.5)\n",
    "          plt.subplot(1,3,2)\n",
    "          plt.imshow(model_training_hr_dataset[i]/2+0.5)\n",
    "          plt.subplot(1,3,3)\n",
    "          plt.imshow(generated[i]/2 + 0.5)\n",
    "          plt.show()\n",
    "\n",
    "\n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                        time.time()-start))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "checkpoint_dir = './ESRGAN_checkpoints/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.model_training.Checkpoint(gen_model_optimizer=gen_model_optimizer,disc_model_optimizer=disc_model_optimizer,gen_model=gen_model,disc_model=disc_model)\n",
    "\n",
    "checkpoint.restore(tf.model_training.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "fit(model_training_lr_dataset,model_training_hr_dataset , EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qKDtfl26r9M"
   },
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "for i in range(0,80):\n",
    "  generated = gen_model(model_training_lr_dataset[10*i:10*i+10])\n",
    "  for j in range(9):\n",
    "      f ,ax = plt.subplots(1,3)\n",
    "      ax[0].imshow(model_training_lr_dataset[j+10*i]/2+0.5)\n",
    "      ax[0].set_title(\"LR image\")\n",
    "      ax[0].axis('off')\n",
    "      ax[1].imshow(model_training_hr_dataset[j+10*i]/2+0.5)\n",
    "      ax[1].set_title(\"HR image\")\n",
    "      ax[1].axis('off')\n",
    "      ax[2].imshow(generated[j]/2 + 0.5)\n",
    "      ax[2].set_title(\"Generated image\")\n",
    "      ax[2].axis('off')\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KyU5c4M-p6ex",
    "outputId": "21d0a345-b870-477c-e360-3a3ef0bcb73f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./ESRGAN_checkpoints/ckpt-4'"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ESRGAN-implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
